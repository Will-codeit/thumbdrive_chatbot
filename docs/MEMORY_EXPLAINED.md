# How We Fit a 40GB Model into 8GB RAM

A technical deep-dive into memory management for running large language models on limited hardware.

---

## ğŸ¤” The Paradox

**Question:** How can a 40GB model file run on a Mac with only 8GB of RAM?

**Answer:** It doesn't all load at once! The model stays on disk, and only small portions are loaded into RAM as needed.

---

## ğŸ”§ The Technology: Memory-Mapped Files (mmap)

### What is mmap?

**Memory-mapped files** (mmap) is an operating system feature that allows programs to treat files on disk as if they were in memory.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Thumb Drive (Physical Storage)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   deepseek-v3-Q4_K_M.gguf         â”‚  â”‚
â”‚  â”‚   Size: 40GB                      â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚   [Model Weights - Layer 1]      â”‚â—„â”€â”
â”‚  â”‚   [Model Weights - Layer 2]      â”‚  â”‚
â”‚  â”‚   [Model Weights - Layer 3]      â”‚  â”‚ mmap maps
â”‚  â”‚   ...                             â”‚  â”‚ file to
â”‚  â”‚   [Model Weights - Layer 80]     â”‚  â”‚ virtual
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ memory
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Only needed pages
                    â”‚ are loaded into RAM
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAM (Physical Memory 8GB)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Active Model Pages (~6-8GB)      â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚  [Layer 5 weights]  â† Currently  â”‚  â”‚
â”‚  â”‚  [Layer 12 weights] â† processing â”‚  â”‚
â”‚  â”‚  [Layer 23 weights] â† these      â”‚  â”‚
â”‚  â”‚  [Context buffer]                â”‚  â”‚
â”‚  â”‚  [Inference state]               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Other RAM: OS, apps, buffers           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How llama.cpp Uses mmap

When llama.cpp starts:

1. **Opens the 40GB file** on the thumb drive
2. **Maps it to virtual memory** (doesn't load it yet!)
3. **As inference runs**, only the needed weights are loaded
4. **OS handles paging** - swapping data in/out of RAM automatically

---

## ğŸ“Š Real Memory Usage Breakdown

### For Q4_K_M Model (40GB file) on 8GB RAM Mac:

```
Component                    Size in RAM    Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active Model Layers          6-8 GB         Only layers being used
KV Cache (context)          ~512 MB         Conversation history
Inference Buffers           ~256 MB         Working memory
llama.cpp Server            ~128 MB         Program itself
Operating System            ~2 GB           macOS background
Other Apps                  Variable        User's programs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL IN USE                ~8 GB           Fits in 8GB!
```

**The other 32GB of model data?** Still on the thumb drive, loaded on-demand.

---

## ğŸ”„ How Inference Works

### Step-by-Step Token Generation:

```
User Input: "Write a Python hello world"

Step 1: Tokenization
   "Write" â†’ Token 5792
   "a" â†’ Token 264
   "Python" â†’ Token 13150
   ...
   â†“ Stored in context buffer (~few KB)

Step 2: Attention Layer 1
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Load Layer 1 weights â”‚ â† mmap loads ~500MB from disk
   â”‚ Process tokens       â”‚ â† Computation happens
   â”‚ Generate output      â”‚ â† Result stored in buffer
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
Step 3: Attention Layer 2
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Layer 1 may stay     â”‚ â† Cached if RAM available
   â”‚ Load Layer 2 weights â”‚ â† mmap loads next ~500MB
   â”‚ Process previous out â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
... Repeat for all 80 layers ...

Step 4: Output
   "print('Hello, World!')"
   
Step 5: Next Token (if continuing)
   - Only small portions re-loaded
   - Recently used layers often still cached
   - Fast subsequent generations
```

---

## ğŸ§® The Math

### Why This Works:

**DeepSeek-V3 Architecture:**
- 671B total parameters (full model)
- 37B active parameters per token (MoE - Mixture of Experts)
- Only ~5% of model is "active" at any time

**Q4_K_M Quantization:**
- Each parameter: ~4 bits (0.5 bytes)
- 37B active params Ã— 0.5 bytes = ~18.5GB theoretical
- With optimizations: ~6-8GB actual RAM usage

**Why not 18.5GB?**
1. **Not all layers load at once** - processed sequentially
2. **Weight sharing** - some weights reused
3. **Sparse activation** - MoE only activates relevant experts
4. **Efficient caching** - frequently used weights stay resident

---

## ğŸ’¾ Apple Silicon Advantage

### Unified Memory Architecture:

```
Traditional Computer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CPU   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   RAM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                   
     â”‚ PCIe Bus          
     â†“                   
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  VRAM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘ Data must copy between RAM and VRAM

Apple Silicon (M1/M2/M3/M4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Unified Memory (8GB)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Shared by CPU and GPU   â”‚  â”‚
â”‚  â”‚  No copying needed!      â”‚  â”‚
â”‚  â”‚  Direct GPU access       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘              â†‘
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”      â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚  CPU  â”‚      â”‚  GPU  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- GPU can access model weights directly from RAM
- No RAMâ†’VRAM copying overhead
- Faster inference on Apple Silicon
- More efficient memory usage

---

## âš¡ GPU Offloading

### What `-ngl 20` Means (8GB RAM Configuration):

```
Model has 80 layers total

CPU Processing:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layers 1-60 (60 layers)        â”‚ â† Processed on CPU
â”‚ Load from disk â†’ Process â†’ Out â”‚   Slower but works
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GPU Processing:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layers 61-80 (20 layers)       â”‚ â† Processed on GPU
â”‚ Stay in unified memory          â”‚   Much faster!
â”‚ Metal acceleration active       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why only 20 layers on GPU with 8GB?**
- GPU layers stay fully resident in RAM
- 20 layers Ã— ~300MB = ~6GB
- Leaves ~2GB for OS and other tasks
- Balance between speed and stability

**With 32GB RAM?**
- Can use `-ngl 99` (all layers on GPU)
- Entire model fits in unified memory
- Maximum performance!

---

## ğŸŒ Performance Impact

### 8GB vs 16GB vs 32GB RAM:

```
Time to Generate 100 Tokens:

8GB RAM (Q3_K_M, 20 GPU layers)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~12 seconds
- Frequent disk access
- Some layers CPU-only
- Still very usable!

16GB RAM (Q4_K_M, 33 GPU layers)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~8 seconds
- More layers cached
- More GPU acceleration
- Smooth experience

32GB RAM (Q5_K_M, 99 GPU layers)
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~5 seconds
- Most model in RAM
- All layers on GPU
- Fast responses

64GB RAM (Q6_K, 99 GPU layers, mlock)
â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~3 seconds
- Entire model locked in RAM
- Maximum quality
- Blazing fast
```

---

## ğŸ” Paging Explained

### What Happens When RAM is Full:

```
1. Initial State (8GB RAM, starting server)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RAM: 2GB free             â”‚
   â”‚ â”œâ”€ macOS: 4GB             â”‚
   â”‚ â”œâ”€ Model cache: 2GB       â”‚
   â”‚ â””â”€ Available: 2GB         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Thumb Drive: Model file ready

2. First Inference (loading Layer 15)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RAM: 500MB free           â”‚
   â”‚ â”œâ”€ macOS: 4GB             â”‚
   â”‚ â”œâ”€ Model cache: 3.5GB     â”‚ â† Layer 15 loaded
   â”‚ â””â”€ Available: 500MB       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Thumb Drive: Reading Layer 15 (500MB)

3. Need Layer 47 (RAM full!)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ macOS pages out old data  â”‚ â† OS evicts unused Layer 15
   â”‚ â”œâ”€ macOS: 4GB             â”‚
   â”‚ â”œâ”€ Model cache: 3.5GB     â”‚ â† Now has Layer 47
   â”‚ â””â”€ Available: 500MB       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Thumb Drive: Reading Layer 47 (500MB)
   
4. Need Layer 15 Again
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Must reload from disk!    â”‚ â† This is the "slowness"
   â”‚ â”œâ”€ macOS: 4GB             â”‚
   â”‚ â”œâ”€ Model cache: 3.5GB     â”‚ â† Back to Layer 15
   â”‚ â””â”€ Available: 500MB       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Thumb Drive: Re-reading Layer 15
   
   âš ï¸ This disk I/O is why more RAM = faster
```

---

## ğŸ¯ Optimization Strategies

### What llama.cpp Does to Minimize RAM:

1. **Quantization** - Reduce precision
   - FP32 (32-bit): 4 bytes per param
   - Q8 (8-bit): 1 byte per param (4x smaller)
   - Q4 (4-bit): 0.5 bytes per param (8x smaller)
   - Q3 (3-bit): 0.375 bytes per param (10.6x smaller)

2. **Layer-by-Layer Processing**
   - Only load current layer
   - Discard after processing (if RAM tight)
   - Sequential processing reduces peak memory

3. **KV Cache Optimization**
   - Only store keys/values for context tokens
   - Configurable max context (2048 vs 16384)
   - Smaller context = less RAM

4. **Batch Size Control**
   - Process fewer tokens simultaneously
   - 8GB: batch_size=256
   - 32GB: batch_size=512
   - Lower batch = less RAM, slightly slower

5. **mlock (if enough RAM)**
   - Locks pages in RAM (prevents swapping)
   - Only on 16GB+ systems
   - Ensures hot paths stay fast

---

## ğŸ“ˆ Real-World Example

### User with 8GB Mac Running Q3_K_M:

```bash
$ ./scripts/run.sh

ğŸ” Checking system requirements...
âœ“ macOS version: 14.5
âœ“ Total RAM: 8GB
âš ï¸  8GB RAM detected - using optimized Q3 model

ğŸš€ Starting DeepSeek-V3 Server...
âš™ï¸  8GB RAM detected - using optimized settings
   Model: Q3_K_M (smaller, faster)
   Context: 2048 tokens
ğŸ“Š Model: models/deepseek-v3-Q3_K_M.gguf
ğŸ’¾ Context: 2048 tokens
ğŸ® GPU Layers: 20

# Server starts...
# Memory usage steady at ~6.5GB
# Thumb drive activity during first few responses
# Then cached layers make it fast!

User: "Write a Python hello world"
# Response in ~3 seconds (first time, loading layers)

User: "Now explain what it does"  
# Response in ~1 second (layers cached!)

User: "Write it in JavaScript too"
# Response in ~1.5 seconds (similar layers reused)
```

---

## ğŸ”¬ Technical Details

### The GGUF File Format:

```
deepseek-v3-Q4_K_M.gguf (40GB file)
â”œâ”€â”€ Header (metadata)
â”‚   â”œâ”€â”€ Model architecture
â”‚   â”œâ”€â”€ Quantization info
â”‚   â””â”€â”€ Tensor locations
â”œâ”€â”€ Tensor Data (39.9GB)
â”‚   â”œâ”€â”€ Layer 0 Attention Weights
â”‚   â”œâ”€â”€ Layer 0 FFN Weights
â”‚   â”œâ”€â”€ Layer 1 Attention Weights
â”‚   â”œâ”€â”€ Layer 1 FFN Weights
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ Layer 79 Weights
â””â”€â”€ Footer (checksums)

Each tensor has:
- Offset in file (byte position)
- Size (bytes)
- Dimensions (shape)
- Type (Q4_K, Q3_K, etc.)

llama.cpp uses these offsets to mmap
only the tensors it needs!
```

---

## ğŸ’¡ Key Takeaways

1. **The model file stays on disk** - 40GB never loads into RAM
2. **Only active portions load** - typically 6-12GB depending on RAM
3. **OS handles paging** - automatically swaps data in/out
4. **More RAM = faster** - less disk I/O, more caching
5. **8GB works!** - with smaller model (Q3) and optimizations
6. **Apple Silicon helps** - unified memory architecture is efficient

---

## ğŸ“ Why This Matters

This technology enables:
- âœ… Running powerful AI on consumer hardware
- âœ… Privacy (no cloud needed)
- âœ… Offline operation (no internet)
- âœ… Cost savings (no API fees)
- âœ… Portability (thumb drive!)

**The magic:** Modern OS memory management + smart quantization + efficient inference = AI on your Mac! ğŸš€

---

**Last Updated:** December 3, 2025
